"""
===============================================================================
File: ukf_aliter.py
Author: Anubhab Ghosh (Feb 2023) | Annotated by ChatGPT (2025)
Adopted from: https://github.com/KalmanNet/KalmanNet_TSP
-------------------------------------------------------------------------------
ğŸ¯ èƒŒæ™¯ (Background)
- ä¸»é¢˜ï¼šéçº¿æ€§çŠ¶æ€ä¼°è®¡ï¼ˆUnscented Kalman Filter, UKFï¼‰åœ¨ Lorenz ç­‰éçº¿æ€§ç³»ç»Ÿä¸Šçš„å®ç°ã€‚
- æ ¸å¿ƒï¼šé€šè¿‡æ— è¿¹å˜æ¢ï¼ˆUnscented Transform, UTï¼‰ä¼ æ’­å‡å€¼ä¸åæ–¹å·®ï¼Œé¿å…ä¸€é˜¶çº¿æ€§åŒ–è¯¯å·®ã€‚

ğŸ“¥ğŸ“¤ è¾“å…¥è¾“å‡ºæ¦‚è§ˆ (I/O Overview)
- System functions:
  - f: éçº¿æ€§çŠ¶æ€è½¬ç§»å‡½æ•°ï¼Œæ¥å£ fx(x, dt) -> x_next
  - h: éçº¿æ€§è§‚æµ‹å‡½æ•°ï¼Œæ¥å£ hx(x) -> z_hat
- Data tensors:
  - X: çœŸå®çŠ¶æ€è½¨è¿¹ï¼Œå½¢çŠ¶ [N, T_x, n_states]
  - Y: è§‚æµ‹åºåˆ—ï¼Œå½¢çŠ¶ [N, T_y, n_obs]
  - è¿”å›ä¼°è®¡ï¼š
      traj_estimated: UKF çš„çŠ¶æ€åéªŒä¼°è®¡ï¼Œå½¢çŠ¶ [N, T_x, n_states]
      Pk_estimated: UKF çš„åéªŒåæ–¹å·®ï¼Œå½¢çŠ¶ [N, T_x, n_states, n_states]
      MSE_UKF_linear_arr.mean(): å¹³å‡ MSEï¼ˆçº¿æ€§å°ºåº¦ï¼‰
      mse_ukf_dB_avg: å¹³å‡ MSEï¼ˆdB å°ºåº¦ï¼‰

ğŸ‘¥ ç›®æ ‡è¯»è€… (Intended Audience)
- æ—¢é¢å‘åˆå­¦è€…ï¼ˆå¸Œæœ›å¿«é€ŸæŒæ¡ UKF çš„æ•°æ®ç»´åº¦ä¸è°ƒç”¨æ–¹å¼ï¼‰ï¼Œä¹Ÿé¢å‘ç ”ç©¶è€…/å·¥ç¨‹å¸ˆï¼ˆå…³æ³¨æ•°å­¦å»ºæ¨¡ä¸å®ç°ç»†èŠ‚ï¼‰ã€‚
- æ³¨é‡Šé‡‡ç”¨ä¸­è‹±æ··æ’ï¼šä¸­æ–‡è§£é‡Šå·¥ç¨‹åŠ¨æœºï¼›è‹±æ–‡ä¿æŒå˜é‡/å‡½æ•°åä¸€è‡´ä»¥ä¾¿æ£€ç´¢ã€‚
===============================================================================
"""

#####################################################
# Creator: Anubhab Ghosh
# Feb 2023
# Adopted from: https://github.com/KalmanNet/KalmanNet_TSP
#####################################################
from filterpy.kalman import UnscentedKalmanFilter, MerweScaledSigmaPoints, JulierSigmaPoints
import torch
from torch import nn
from timeit import default_timer as timer
# from utils.utils import dB_to_lin, mse_loss
from scipy.linalg import expm
# from parameters import delta_t, J_test
import numpy as np
import math


def A_fn_exact(z, dt):
    """
    Compute exact one-step flow of the continuous-time Lorenz system using matrix exponential.
    ä½¿ç”¨çŸ©é˜µæŒ‡æ•°å¯¹è¿ç»­æ—¶é—´ Lorenz ç³»ç»Ÿè¿›è¡Œä¸€é˜¶ç²¾ç¡®ç¦»æ•£åŒ–ï¼Œå¹¶ä½œç”¨äºçŠ¶æ€å‘é‡ã€‚

    Args:
        z (np.ndarray): å½“å‰çŠ¶æ€å‘é‡ current state, shape [3,], order [x, y, z].
        dt (float): é‡‡æ ·å‘¨æœŸ sampling time step.

    Returns:
        np.ndarray: ä¸‹ä¸€æ—¶åˆ»çŠ¶æ€çš„çº¿æ€§è¿‘ä¼¼ç»“æœï¼ˆé€šè¿‡å±€éƒ¨çº¿æ€§åŒ–çŸ©é˜µæŒ‡æ•°å¾—åˆ°ï¼‰,
                    shape [3,].

    Tensor Dimensions:
        z âˆˆ R^[3], dt âˆˆ R, return âˆˆ R^[3]

    Math Notes:
        Continuous-time linearization around state z:
            A(z) =
                [[-10,   10,      0],
                 [ 28,   -1,   -z_x],
                 [  0,   z_x,  -8/3]]
        Then state propagation (local linear model):
            x_{k+1} â‰ˆ expm(A(z_k) * dt) @ z_k
        è¯¥å‡½æ•°å°† A(z) çš„çŸ©é˜µæŒ‡æ•°ç›´æ¥ä¹˜ä»¥å½“å‰çŠ¶æ€ï¼Œå®ç°ä¸€é˜¶ç²¾ç¡®ç¦»æ•£åŒ–çš„è¿‘ä¼¼ã€‚
    """
    # ==========================================================================
    # STEP 01: è®¡ç®—çŠ¶æ€ç›¸å…³çš„é›…å¯æ¯”çŸ©é˜µ A(z)
    # --------------------------------------------------------------------------
    # Lorenz ç³»ç»Ÿçš„çº¿æ€§åŒ–çŸ©é˜µ A ä¾èµ–äºå½“å‰ x åˆ†é‡ (z[0]); è¿™é‡Œä½¿ç”¨ç»å…¸å‚æ•°ã€‚
    # ==========================================================================
    return expm(np.array([
                    [-10, 10, 0],
                    [28, -1, -z[0]],
                    [0, z[0], -8.0/3]
                ]) * dt) @ z


'''
def f_lorenz_danse(x, dt):
    # NOTE: This alternative formulation uses a Taylor expansion to build F.
    # è¯´æ˜ï¼šè¯¥æ®µä¸ºæ›¿ä»£æ€§å®ç°ï¼ˆä¿ç•™ä¸ºæ³¨é‡Šï¼‰ï¼Œé€šè¿‡æ³°å‹’å±•å¼€è¿‘ä¼¼çŠ¶æ€è½¬ç§»çŸ©é˜µ Fã€‚
    # ä¸å»ºè®®åœ¨ç”Ÿäº§ä¸­ç›´æ¥å¯ç”¨ï¼Œå› å­˜åœ¨ç¡¬ç¼–ç ä¾èµ–å’Œæ½œåœ¨è®¾å¤‡/æ¢¯åº¦é—®é¢˜ã€‚

    x = torch.from_numpy(x).type(torch.FloatTensor)
    B = torch.Tensor([[[0,  0, 0],[0, 0, -1],[0,  1, 0]], torch.zeros(3,3), torch.zeros(3,3)]).type(torch.FloatTensor)
    C = torch.Tensor([[-10, 10,    0],
                    [ 28, -1,    0],
                    [  0,  0, -8/3]]).type(torch.FloatTensor)
    #A = torch.add(torch.einsum('nhw,wa->nh', B, x).T,C)
    A = torch.einsum('kn,nij->ij',x.reshape((1,-1)),B)
    #A = torch.reshape(torch.matmul(B, x),(3,3)).T # For KalmanNet
    A += C
    #delta = delta_t # Hardcoded for now
    # Taylor Expansion for F
    F = torch.eye(3)
    J = J_test # Hardcoded for now
    for j in range(1,J+1):
        F_add = (torch.matrix_power(A*delta_t, j)/math.factorial(j))
        F = torch.add(F, F_add)
    return torch.matmul(F, x).numpy()
'''


class UKF_Aliter(nn.Module):
    """This class implements an Unscented Kalman Filter (UKF) using FilterPy under a PyTorch wrapper.
    è¯¥ç±»å°è£… FilterPy çš„ UKFï¼Œåœ¨ PyTorch è®¾å¤‡ä¸Šç®¡ç†å¼ é‡ä¸æ‰¹å¤„ç†æ¥å£ï¼ˆä¸æ”¹å˜ FilterPy å†…éƒ¨å®ç°ï¼‰ã€‚

    Args:
        n_states (int): çŠ¶æ€ç»´åº¦ number of states n_x.
        n_obs (int): è§‚æµ‹ç»´åº¦ number of observations n_z.
        f (callable): çŠ¶æ€è½¬ç§»å‡½æ•° fx(x, dt) -> x_nextï¼Œéçº¿æ€§ã€‚
        h (callable): è§‚æµ‹å‡½æ•° hx(x) -> z_hatï¼Œéçº¿æ€§ã€‚
        Q (np.ndarray | None): è¿‡ç¨‹å™ªå£°åæ–¹å·® process noise cov, shape [n_states, n_states].
        R (np.ndarray | None): è§‚æµ‹å™ªå£°åæ–¹å·® measurement noise cov, shape [n_obs, n_obs].
        kappa (float): UT è¶…å‚æ•°ï¼Œå†³å®š sigma ç‚¹åˆ†å¸ƒçš„ç¦»æ•£åº¦ã€‚
        alpha (float): UT è¶…å‚æ•°ï¼Œé€šå¸¸å– 1e-3~0.5ï¼Œå°åˆ™ sigma ç‚¹æ›´é è¿‘å‡å€¼ã€‚
        beta (float): UT è¶…å‚æ•°ï¼Œé’ˆå¯¹é«˜æ–¯åˆ†å¸ƒå¸¸å– 2ã€‚
        n_sigma (int | None): æœªä½¿ç”¨çš„å ä½å‚æ•°ï¼ˆä¿ç•™å…¼å®¹ï¼‰ã€‚
        delta_t (float): é‡‡æ ·å‘¨æœŸ sampling time step ç”¨äº FilterPy UKFã€‚
        inverse_r2_dB (float | None): è‹¥æä¾›ï¼Œä¸ nu_dB å…±åŒå†³å®š Q å’Œ Rï¼ˆä»¥ dB å°ºåº¦ç»™å‡ºï¼‰ã€‚
        nu_dB (float | None): å™ªå£°åŠŸç‡å·®ï¼ˆdBï¼‰ï¼Œç”¨äºä» inverse_r2_dB æ¨å‡º q2ã€‚
        device (str): PyTorch è®¾å¤‡, e.g., "cpu" æˆ– "cuda".
        init_cond (torch.Tensor | None): åˆå§‹æ¡ä»¶ï¼ˆå½“å‰æœªä½¿ç”¨ï¼›ä¿ç•™æ¥å£ï¼‰ã€‚

    Attributes:
        device (str): å½“å‰è®¾å¤‡ã€‚
        n_states (int), n_obs (int): ç»´åº¦ä¿¡æ¯ã€‚
        f_k, h_k (callable): éçº¿æ€§ç³»ç»Ÿå‡½æ•°ã€‚
        Q_k, R_k (torch.FloatTensor): è¿‡ç¨‹/è§‚æµ‹å™ªå£°åæ–¹å·®ï¼ˆé©»ç•™åœ¨ deviceï¼‰ã€‚
        sigma_points (MerweScaledSigmaPoints): æ— è¿¹å˜æ¢ sigma ç‚¹ç”Ÿæˆå™¨ã€‚
        ukf (filterpy.kalman.UnscentedKalmanFilter): FilterPy UKF å®ä¾‹ã€‚

    Tensor Dimensions:
        Q_k âˆˆ R^[n_states, n_states], R_k âˆˆ R^[n_obs, n_obs]
        ukf.x âˆˆ R^[n_states], ukf.P âˆˆ R^[n_states, n_states]

    Math Notes:
        UT é€šè¿‡é€‰å– 2n+1 ä¸ª sigma ç‚¹ {chi_i} åŠæƒé‡ {W_i} æ¥ä¼ æ’­éçº¿æ€§ï¼š
            x_pred   = sum_i W_i^m * f(chi_i, dt)
            P_pred   = sum_i W_i^c * (f(chi_i) - x_pred)(...)^T + Q
            z_pred   = sum_i W_i^m * h(chi_i)
            P_zz     = sum_i W_i^c * (h(chi_i) - z_pred)(...)^T + R
            P_xz     = sum_i W_i^c * (chi_i - x_pred)(h(chi_i) - z_pred)^T
            K        = P_xz @ inv(P_zz)
            x_post   = x_pred + K @ (z - z_pred)
            P_post   = P_pred - K @ P_zz @ K^T
    """
    def __init__(self, n_states, n_obs, f=None, h=None, Q=None, R=None, kappa=-1,
                alpha=0.1, beta=2, n_sigma=None, delta_t=1.0, inverse_r2_dB=None,
                nu_dB=None, device='cpu', init_cond=None):
        super(UKF_Aliter, self).__init__()

        # ==========================================================================
        # STEP 01: è®¾å¤‡ä¸ç³»ç»Ÿç»´åº¦åˆå§‹åŒ– (Device & dimensions)
        # ==========================================================================
        self.device = device
        self.n_states = n_states
        self.n_obs = n_obs
        self.f_k = f
        self.h_k = h

        # ==========================================================================
        # STEP 02: å™ªå£°åæ–¹å·®æ¥æº (Noise covariance setup)
        # --------------------------------------------------------------------------
        # è‹¥æä¾› inverse_r2_dB ä¸ nu_dB ä¸” Q/R æœªæ˜¾å¼ç»™å‡ºï¼Œåˆ™ä½¿ç”¨ dB è§„åˆ™æ„é€ ã€‚
        # è®¾è®¡åŸå› ï¼šä¾¿äºä»…é€šè¿‡ä¿¡å™ªå‚æ•°å¿«é€Ÿè®¾å®šæ»¤æ³¢å™¨è¶…å‚ã€‚
        # r2 = 1 / SNR_linear; q2 = 10^((nu_dB - inverse_r2_dB)/10)
        # ==========================================================================
        if (not inverse_r2_dB is None) and (not nu_dB is None) and (Q is None) and (R is None):
            r2 = 1.0 / dB_to_lin(inverse_r2_dB)
            q2 = dB_to_lin(nu_dB - inverse_r2_dB)
            Q = q2 * np.eye(self.n_states)
            R = r2 * np.eye(self.n_obs)

        # å°† numpy è½¬ä¸ºé©»ç•™åœ¨ device çš„ torch å¼ é‡
        self.Q_k = self.push_to_device(Q)  # è¿‡ç¨‹å™ªå£°åæ–¹å·® Q
        self.R_k = self.push_to_device(R)  # è§‚æµ‹å™ªå£°åæ–¹å·® R

        # ==========================================================================
        # STEP 03: æ— è¿¹å˜æ¢è¶…å‚æ•°ä¸ sigma ç‚¹ (UT hyperparams & sigma points)
        # ==========================================================================
        self.kappa = kappa
        self.alpha = alpha
        self.beta = beta
        self.get_sigma_points()  # è®¾ç½® self.sigma_points

        # self.init_cond = init_cond  # ä¿ç•™æ¥å£ï¼ˆå½“å‰æœªå¯ç”¨ï¼‰

        # ==========================================================================
        # STEP 04: æ„å»º FilterPy çš„ UKF å®ä¾‹ (Instantiate FilterPy UKF)
        # ==========================================================================
        self.delta_t = delta_t
        self.ukf = UnscentedKalmanFilter(dim_x=self.n_states, dim_z=self.n_obs, dt=self.delta_t,
                                        fx=self.f_k, hx=self.h_k, points=self.sigma_points)
        # å°†å™ªå£°ä¸åˆå€¼åŒæ­¥åˆ° FilterPy UKF (FilterPy æœŸæœ› numpy)
        self.ukf.R = self.R_k.numpy()
        self.ukf.Q = self.Q_k.numpy()
        self.ukf.x = torch.ones((self.n_states,)).numpy()
        self.ukf.P = (torch.eye(self.n_states) * 1e-5).numpy()
        return None

    def initialize(self):
        """
        Reset internal UKF posterior mean and covariance.
        é‡ç½® UKF çš„å†…éƒ¨åéªŒå‡å€¼ä¸åæ–¹å·®ï¼ˆæ¯ä¸ªæ ·æœ¬åºåˆ—å¼€å§‹æ—¶è°ƒç”¨ï¼‰ã€‚

        Args:
            None

        Returns:
            None

        Tensor Dimensions:
            ukf.x âˆˆ R^[n_states], ukf.P âˆˆ R^[n_states, n_states]

        Math Notes:
            x_post_0 = 1 (per component)
            P_post_0 = 1e-5 * I
            å°æ–¹å·®åˆå€¼æ„å‘³ç€å¯¹åˆå§‹çŠ¶æ€æœ‰å¼ºå…ˆéªŒçº¦æŸï¼›è‹¥å…ˆéªŒä¸å‡†ï¼Œå¯èƒ½æ”¶æ•›è¾ƒæ…¢ã€‚
        """
        # ==========================================================================
        # STEP 01: é‡ç½®åéªŒ
        # ==========================================================================
        self.ukf.x = torch.ones((self.n_states,)).numpy()
        self.ukf.P = (torch.eye(self.n_states) * 1e-5).numpy()

    def push_to_device(self, x):
        """
        Convert a numpy array to torch.FloatTensor on the configured device.
        å°† numpy æ•°ç»„è½¬æ¢ä¸ºä½äºæŒ‡å®š device çš„ FloatTensorã€‚

        Args:
            x (np.ndarray): è¾“å…¥çŸ©é˜µ/å‘é‡ input array.

        Returns:
            torch.FloatTensor: ä½äº self.device çš„å¼ é‡ã€‚

        Tensor Dimensions:
            ä¿æŒä¸è¾“å…¥ç›¸åŒçš„å½¢çŠ¶ï¼›ä»…æ”¹å˜ç±»å‹ä¸è®¾å¤‡ã€‚

        Math Notes:
            æ— æ•°å­¦å˜æ¢ï¼Œä»…æ•°æ®ç±»å‹ä¸å†…å­˜ä½ç½®è½¬æ¢ã€‚
        """
        # ==========================================================================
        # STEP 01: ç±»å‹ä¸è®¾å¤‡è¿ç§»
        # ==========================================================================
        return torch.from_numpy(x).type(torch.FloatTensor).to(self.device)

    def get_sigma_points(self):
        """
        Construct sigma point generator (MerweScaledSigmaPoints).
        æ„é€  Merwe ç¼©æ”¾ sigma ç‚¹ç”Ÿæˆå™¨ã€‚

        Args:
            None

        Returns:
            None

        Math Notes:
            MerweScaledSigmaPoints é€šè¿‡ (alpha, beta, kappa) æ§åˆ¶ç‚¹åˆ†å¸ƒä¸æƒé‡ã€‚
            å¯¹é«˜æ–¯åˆ†å¸ƒï¼Œbeta=2 é€šå¸¸æœ€ä¼˜ï¼›alpha æ§åˆ¶åˆ†å¸ƒåŠå¾„ï¼›kappa å½±å“ç¦»æ•£åº¦ã€‚
        """
        # ==========================================================================
        # STEP 01: åˆå§‹åŒ– sigma ç‚¹ç­–ç•¥
        # ==========================================================================
        self.sigma_points = MerweScaledSigmaPoints(self.n_states, alpha=self.alpha, beta=self.beta, kappa=self.kappa)

    def run_mb_filter(self, X, Y, U=None):
        """
        Run UKF over a mini-batch of sequences and compute MSE.
        åœ¨å°æ‰¹é‡åºåˆ—ä¸Šè¿è¡Œ UKFï¼Œå¹¶è¿”å›è½¨è¿¹ä¼°è®¡åŠ MSE ç»Ÿè®¡ã€‚

        Args:
            X (torch.Tensor): çœŸå®çŠ¶æ€è½¨è¿¹ ground-truth states,
                shape [N, T_x, n_states] æˆ– [T_x, n_states]ï¼ˆå°†è‡ªåŠ¨å‡ç»´ä¸º N=1ï¼‰ã€‚
            Y (torch.Tensor): è§‚æµ‹åºåˆ— measurements,
                shape [N, T_y, n_obs] æˆ– [T_y, n_obs]ï¼ˆå°†è‡ªåŠ¨å‡ç»´ä¸º N=1ï¼‰ã€‚
            U (torch.Tensor | None): æ§åˆ¶è¾“å…¥ control inputsï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™æ¥å£ï¼‰ã€‚

        Returns:
            tuple:
                traj_estimated (torch.FloatTensor): åéªŒçŠ¶æ€ä¼°è®¡, shape [N, T_x, n_states].
                Pk_estimated (torch.FloatTensor): åéªŒåæ–¹å·®ä¼°è®¡, shape [N, T_x, n_states, n_states].
                MSE_UKF_linear_arr.mean() (torch.FloatTensor): å¹³å‡ MSEï¼ˆçº¿æ€§å°ºåº¦æ ‡é‡ï¼‰ã€‚
                mse_ukf_dB_avg (torch.FloatTensor): å¹³å‡ MSEï¼ˆdB å°ºåº¦æ ‡é‡ï¼‰ã€‚

        Tensor Dimensions:
            N: batch size; T_x: çŠ¶æ€åºåˆ—é•¿åº¦; T_y: è§‚æµ‹åºåˆ—é•¿åº¦
            n_states: çŠ¶æ€ç»´åº¦ï¼›n_obs: è§‚æµ‹ç»´åº¦

        Math Notes:
            å¤„ç†æµç¨‹ï¼š
                for each sequence i:
                    initialize posterior (x_post_0, P_post_0)
                    for k in 0..T_y-1:
                        predict via UT: (x_pred, P_pred)
                        update with y[k]: (x_post, P_post)
                    compute MSE_i = mse(X_i[1:], traj_i[1:])
            è¿™é‡Œå°†ç¬¬ä¸€æ­¥ä¼°è®¡ä¸è§‚æµ‹å¯¹é½ï¼Œä» k=0 å¼€å§‹ï¼Œæœ€åè¯„ä¼°ä½¿ç”¨ 1: å¯¹é½ç´¢å¼•ã€‚
        """
        # ==========================================================================
        # STEP 00: ç»´åº¦è§£æä¸å¼ é‡å‡†å¤‡ (Shape parsing & allocation)
        # ==========================================================================
        _, Ty, dy = Y.shape
        _, Tx, dx = X.shape

        if len(Y.shape) == 3:
            N, T, d = Y.shape
        elif len(Y.shape) == 2:
            T, d = Y.shape
            N = 1
            Y = Y.reshape((N, Ty, d))

        # ä¼°è®¡è½¨è¿¹ä¸åæ–¹å·®ç¼“å­˜ï¼ˆé©»ç•™ deviceï¼‰
        traj_estimated = torch.zeros((N, Tx, self.n_states), device=self.device).type(torch.FloatTensor)
        Pk_estimated = torch.zeros((N, Tx, self.n_states, self.n_states), device=self.device).type(torch.FloatTensor)

        # æ‰¹å†…æ¯ä¸ªåºåˆ—çš„ MSEï¼ˆçº¿æ€§å°ºåº¦ï¼‰
        MSE_UKF_linear_arr = torch.zeros((N,)).type(torch.FloatTensor)
        # points = JulierSigmaPoints(n=SysModel.m)  # å¤‡ç”¨ sigma ç‚¹æ–¹æ¡ˆï¼ˆæœªå¯ç”¨ï¼‰

        # ==========================================================================
        # STEP 01: æ‰¹å¤„ç†ä¸»å¾ªç¯ (Main batch loop)
        # ==========================================================================
        start = timer()
        for i in range(0, N):
            # æ¯ä¸ªæ ·æœ¬åºåˆ—é‡ç½®æ»¤æ³¢å™¨åˆå€¼
            self.initialize()
            # if self.init_cond is not None:
            #     self.ukf.x = torch.unsqueeze(self.init_cond[i, :], 1).numpy()

            # ----------------------------------------------------------------------
            # STEP 01a: æ—¶é—´æ­¥è¿­ä»£ (Time-step loop)
            # ----------------------------------------------------------------------
            for k in range(0, Ty):
                # å…ˆéªŒé¢„æµ‹ï¼šUT ä¼ æ’­å‡å€¼ä¸åæ–¹å·®
                self.ukf.predict()
                # è§‚æµ‹æ›´æ–°ï¼šèåˆå½“å‰è§‚æµ‹ Y[i, k, :]
                self.ukf.update(Y[i, k, :].numpy())

                # è®°å½•åéªŒå‡å€¼ä¸åæ–¹å·®ï¼ˆk+1 å¯¹é½åˆ° X çš„ç´¢å¼•æ–¹æ¡ˆï¼‰
                traj_estimated[i, k+1, :] = torch.from_numpy(self.ukf.x)
                Pk_estimated[i, k+1, :, :] = torch.from_numpy(self.ukf.P)

            # ä»¥å¯¹é½åçš„æ—¶åºçª—å£è¯„ä¼° MSE
            # MSE_UKF_linear_arr[i] = mse_loss(traj_estimated[i], X[i]).item()
            MSE_UKF_linear_arr[i] = mse_loss(X[i, 1:, :], traj_estimated[i, 1:, :]).mean().item()
            # print("ukf, sample: {}, mse_loss: {}".format(i+1, MSE_UKF_linear_arr[i]))

        end = timer()
        t = end - start  # è¿è¡Œæ—¶é•¿ï¼Œå¯ç”¨äºæ—¥å¿—
        # print("Inference Time:", t)

        # ==========================================================================
        # STEP 02: ç»Ÿè®¡ä¸æ—¥å¿— (Statistics & logging)
        # ==========================================================================
        # MSE_UKF_linear_avg = torch.mean(MSE_UKF_linear_arr)
        # MSE_UKF_dB_avg = 10 * torch.log10(MSE_UKF_linear_avg)
        # MSE_UKF_linear_std = torch.std(MSE_UKF_linear_arr, unbiased=True)
        # MSE_UKF_dB_std = 10 * torch.log10(MSE_UKF_linear_std.abs())

        mse_ukf_dB_avg = torch.mean(10 * torch.log10(MSE_UKF_linear_arr), dim=0)
        print("UKF - MSE LOSS:", mse_ukf_dB_avg, "[dB]")
        print("UKF - MSE STD:", torch.std(10 * torch.log10(MSE_UKF_linear_arr), dim=0), "[dB]")

        # ==========================================================================
        # STEP 03: è¿”å›ç»“æœ (Return results)
        # ==========================================================================
        return traj_estimated, Pk_estimated, MSE_UKF_linear_arr.mean(), mse_ukf_dB_avg

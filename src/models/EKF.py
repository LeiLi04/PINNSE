"""
===============================================================================
File: ekf_lorenz.py
Author: You (with ChatGPT assist, 2025)

ğŸ¥ Whatâ€™s new vs EKF_Aliter:
- ä¸å†ä¾èµ– FilterPyï¼›å…¨éƒ¨ç”¨ PyTorch/Numpy è‡ªå·±å®ç°ï¼ˆä¾¿äº CUDAã€å¯æ§æ•°å€¼ç¨³å®šï¼‰ã€‚
- ç»Ÿä¸€æ”¯æŒ dB å‚æ•°ï¼šinverse_r2_dB ä¸ nu_dB -> R, Qã€‚
- é¢„æµ‹æ­¥å¯é€‰ä¸€é˜¶/äºŒé˜¶ç¦»æ•£åŒ–ï¼ˆorder=1/2ï¼‰ï¼›æ›´æ–°æ­¥ç”¨ Joseph å½¢å¼ã€‚
- Cholesky ç™½åŒ–æ±‚è§£ S^{-1}ï¼ˆä¸æ˜¾å¼æ±‚é€†ï¼‰ï¼Œå¯é€‰è¾“å‡ºç™½åŒ–åˆ›æ–°/ NIS ä¾¿äºè¯Šæ–­ã€‚
===============================================================================
"""

import math
import numpy as np
import torch
from torch import nn

# ---------- utils ----------
def dB_to_lin(x_dB: float) -> float:
    return 10.0 ** (x_dB / 10.0)

def lin_to_dB(x_lin: float, eps: float = 1e-12) -> float:
    return 10.0 * np.log10(max(x_lin, eps))

def mse_loss(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    return torch.mean((a - b) ** 2)

#   ------- Numerical Jacobian (Central Difference) ----------
def numerical_jacobian_central(func, x: np.ndarray, dt: float = None, eps: float = 1e-5):
    x = np.asarray(x, dtype=float)
    n = x.size
    fx = func(x, dt) if dt is not None else func(x)
    fx = np.asarray(fx, dtype=float)
    m = fx.size
    J = np.zeros((m, n), dtype=float)
    for i in range(n):
        x_f = x.copy(); x_b = x.copy()
        x_f[i] += eps;   x_b[i] -= eps
        f_f = func(x_f, dt) if dt is not None else func(x_f)
        f_b = func(x_b, dt) if dt is not None else func(x_b)
        J[:, i] = (np.asarray(f_f) - np.asarray(f_b)) / (2.0 * eps)
    return J

# ---------- Lorenz-63 dynamics + Jacobian ----------
def lorenz_f(x, sigma=10.0, rho=28.0, beta=8.0/3.0):
    x1, x2, x3 = x
    return np.array([
        sigma * (x2 - x1),
        x1 * (rho - x3) - x2,
        x1 * x2 - beta * x3
    ], dtype=float)

def lorenz_jacobian(x, sigma=10.0, rho=28.0, beta=8.0/3.0):
    x1, x2, x3 = x
    return np.array([
        [-sigma,     sigma,   0.0],
        [ rho - x3, -1.0,   -x1  ],
        [   x2,       x1,   -beta]
    ], dtype=float)

# ---------- EKF class ----------
class EKF_Lorenz(nn.Module):
    """
    Extended Kalman Filter for Lorenz-63.
    - f, h: nonlinear transition/measurement. é»˜è®¤ h(x)=H xã€‚
    - F_jac, H_jac: Jacobians. é»˜è®¤ Lorenz çš„è§£æ Jc ä¸å¸¸æ•° Hã€‚
    - order=1/2: é¢„æµ‹æ­¥çš„ä¸€é˜¶/äºŒé˜¶ç¦»æ•£è¿‘ä¼¼ã€‚
    - Joseph form + Cholesky solve.

    Q/R è®¾å®šï¼š
      è‹¥ä¼ å…¥ inverse_r2_dB ä¸ nu_dBï¼Œåˆ™ï¼š
        r^2 = 10^(-inverse_r2_dB/10),  q^2 = 10^((nu_dB - inverse_r2_dB)/10),
        R = r^2 I, Q = q^2 I
      å¦åˆ™ä½¿ç”¨ Q, R æ˜¾å¼å‚æ•°ã€‚
    """
    def __init__(self, n_states=3, n_obs=3,
                 f=lorenz_f, h=None,
                 F_jacobian=lorenz_jacobian, H_mat=None, H_jacobian=None,
                 Q=None, R=None, delta_t=0.01,
                 inverse_r2_dB=None, nu_dB=None,
                 order=1, device="cpu", jitter=1e-6):
        super().__init__()
        self.nx = n_states
        self.nz = n_obs
        self.f = f
        self.h = h if h is not None else (lambda x: (H_mat @ x))
        self.F_jac = F_jacobian
        self.H_jac = H_jacobian if H_jacobian is not None else (lambda x: H_mat)
        self.dt = float(delta_t)
        self.order = int(order)
        self.device = device
        self.jitter = float(jitter)

        # H é»˜è®¤å•ä½é˜µ
        if H_mat is None:
            H_mat = np.eye(self.nz, self.nx, dtype=float)
        self.H = H_mat.astype(float)

        # ä» dB å»ºç«‹ Q/Rï¼ˆè‹¥æœªæä¾›ï¼‰
        if (inverse_r2_dB is not None) and (nu_dB is not None) and (Q is None) and (R is None):
            r2 = 10.0 ** (-inverse_r2_dB / 10.0)
            q2 = 10.0 ** ((nu_dB - inverse_r2_dB) / 10.0)
            Q = q2 * np.eye(self.nx)
            R = r2 * np.eye(self.nz)

        assert Q is not None and R is not None, "Q/R æœªæä¾›ï¼Œä¹Ÿæœªä» dB å‚æ•°æ¨å¯¼ã€‚"

        self.Q = torch.from_numpy(np.asarray(Q)).float().to(device)
        self.R = torch.from_numpy(np.asarray(R)).float().to(device)
        self.H_torch = torch.from_numpy(self.H).float().to(device)

        # æ»¤æ³¢å†…éƒ¨æ€
        self.x = torch.ones(self.nx, device=device).float()
        self.P = torch.eye(self.nx, device=device).float() * 1e-5

    # ----- åˆå§‹åŒ– -----
    def initialize(self, x0=None, P0=None):
        if x0 is None:
            self.x = torch.ones(self.nx, device=self.device).float()
        else:
            self.x = torch.from_numpy(np.asarray(x0)).float().to(self.device)
        if P0 is None:
            self.P = torch.eye(self.nx, device=self.device).float() * 1e-5
        else:
            self.P = torch.from_numpy(np.asarray(P0)).float().to(self.device)


    # =========================Transition State===================================================
    # ----- é¢„æµ‹ -----
    # ---------- EKF Prediction step ----------
    # æ•°å­¦å…¬å¼è¯´æ˜ï¼š
    #
    # çŠ¶æ€é¢„æµ‹ï¼š
    #   ä¸€é˜¶è¿‘ä¼¼ (order=1):
    #       x_{k+1} â‰ˆ x_k + Î”t f(x_k)
    #   äºŒé˜¶è¿‘ä¼¼ (order=2):
    #       x_{k+1} â‰ˆ x_k + Î”t f(x_k) + 1/2 Î”t^2 J(x_k) f(x_k)
    #
    # çŠ¶æ€è½¬ç§»çŸ©é˜µ Fï¼š
    #   ä¸€é˜¶è¿‘ä¼¼ (order=1):
    #       F_k â‰ˆ I + Î”t J(x_k)
    #   äºŒé˜¶è¿‘ä¼¼ (order=2):
    #       F_k â‰ˆ I + Î”t J(x_k) + 1/2 (Î”t J(x_k))^2
    #
    # åæ–¹å·®ä¼ æ’­ï¼š
    #       P_{k+1} = F_k P_k F_k^T + Q
    #
    # å…¶ä¸­ï¼š
    #   f(x)   : è¿ç»­æ—¶é—´åŠ¨åŠ›å­¦ (Lorenz-63)
    #   J(x)   : f(x) çš„ Jacobian
    #   Î”t     : æ—¶é—´æ­¥é•¿ self.dt
    #   Q      : è¿‡ç¨‹å™ªå£°åæ–¹å·®
    def predict_step(self):
        x_np = self.x.detach().cpu().numpy()
        f = self.f(x_np)                       # R^nx
        J = self.F_jac(x_np)                  # (nx,nx)
        I = torch.eye(self.nx, device=self.device).float()
        J_t = torch.from_numpy(J).float().to(self.device)

        if self.order == 1:
            F = I + self.dt * J_t  
            x_pred = self.x + self.dt * torch.from_numpy(f).float().to(self.device)
        elif self.order == 2:
            Jdt = self.dt * J_t
            F = I + Jdt + 0.5 * (Jdt @ Jdt)
            x_pred = self.x + self.dt * torch.from_numpy(f).float().to(self.device) \
                     + 0.5 * (self.dt**2) * (J_t @ torch.from_numpy(f).float().to(self.device))
        else:
            raise ValueError("order must be 1 or 2")

        P_pred = F @ self.P @ F.T + self.Q
        self.x = x_pred
        self.P = 0.5 * (P_pred + P_pred.T)    # å¯¹ç§°åŒ–ï¼Œé¿å…æ•°å€¼æ¼‚ç§»

        return F  # ä¾¿äºè°ƒè¯•/ä¿å­˜

    # ----- æ›´æ–°ï¼ˆJoseph + Choleskyï¼‰ -----
    # ---------- åæ–¹å·®æ›´æ–° (Joseph form) ----------
    # æ•°å­¦å…¬å¼ï¼š
    #
    #   P_k+ = (I - K_k H_k) P_k- (I - K_k H_k)^T + K_k R_k K_k^T
    #
    # å…¶ä¸­ï¼š
    #   P_k- : å…ˆéªŒåæ–¹å·®
    #   P_k+ : åéªŒåæ–¹å·®
    #   K_k  : å¡å°”æ›¼å¢ç›Š
    #   H_k  : è§‚æµ‹çŸ©é˜µ
    #   R_k  : è§‚æµ‹å™ªå£°åæ–¹å·®

    def update_step(self, z_k: np.ndarray, return_diag=False):
        H = self.H_torch
        z_pred = H @ self.x if self.h is None else torch.from_numpy(self.h(self.x.detach().cpu().numpy())).float().to(self.device)
        innov = torch.from_numpy(np.asarray(z_k)).float().to(self.device) - z_pred

        S = H @ self.P @ H.T + self.R
        S = 0.5 * (S + S.T) + self.jitter * torch.eye(self.nz, device=self.device)
        L = torch.linalg.cholesky(S)
        
        # è®¡ç®— Kalman å¢ç›Š
        # K = P H^T S^{-1} via Cholesky
        HP = H @ self.P                      # (nz,nx)
        U = torch.cholesky_solve(HP, L)      # S^{-1} (H P)
        K = U.T                              # (nx,nz)

        # æ›´æ–°
        self.x = self.x + K @ innov
        I = torch.eye(self.nx, device=self.device)
        P_post = (I - K @ H) @ self.P @ (I - K @ H).T + K @ self.R @ K.T
        self.P = 0.5 * (P_post + P_post.T)

        if return_diag:
            # ç™½åŒ–åˆ›æ–°ä¸ NISï¼ˆä¾¿äºè¯Šæ–­ï¼‰
            nu = torch.cholesky_solve(innov.unsqueeze(1), L).squeeze(1)  # S^{-1} innov
            NIS = (innov * nu).sum()
            return K, L, innov, NIS
        return K

    # ----- è·‘ä¸€æ¡åºåˆ— -----
    @torch.no_grad()
    def filter_sequence(self, Y: torch.Tensor, X_gt: torch.Tensor = None):
        """
        Y: [T, nz];  X_gt: [T, nx] (optional, ä»…ç”¨äºè¯„ä¼°)
        è¿”å›ï¼š
          X_hat: [T, nx]  åéªŒå‡å€¼
          P_hat: [T, nx, nx]  åéªŒåæ–¹å·®
          stats: dict (å¯å« MSE/NIS)
        çº¦å®šï¼šåœ¨ t=0 ä½¿ç”¨åˆå§‹åŒ– (x0,P0)ï¼Œä»ç¬¬ä¸€å¸§è§‚æµ‹ Y[0] å¼€å§‹ predict+updateã€‚
        """
        if Y.dim() == 1:
            Y = Y.unsqueeze(0)
        T = Y.shape[0]

        X_hat = torch.zeros((T, self.nx), device=self.device).float()
        P_hat = torch.zeros((T, self.nx, self.nx), device=self.device).float()
        nis_list = []

        for t in range(T):
            self.predict_step()
            _, L, innov, NIS = self.update_step(Y[t].cpu().numpy(), return_diag=True)
            nis_list.append(NIS.item())
            X_hat[t] = self.x
            P_hat[t] = self.P

        stats = {}
        if X_gt is not None:
            X_gt = X_gt.to(self.device)
            mse_lin = mse_loss(X_gt, X_hat)
            stats["mse_lin"] = mse_lin.item()
            stats["mse_dB"] = lin_to_dB(mse_lin.item())
        stats["NIS_mean"] = float(np.mean(nis_list))
        return X_hat, P_hat, stats


    @staticmethod
    @torch.no_grad()
    def run_ekf_on_dataset(
        Y,                      # np.array [N, T, n_obs]
        X_gt=None,              # np.array [N, T, n_states]ï¼ˆè¯„ä¼°ç”¨ï¼Œå¯ Noneï¼‰
        H=None,                 # np.array [n_obs, n_states]ï¼Œé»˜è®¤ I
        delta_t=0.01,           # æ—¶é—´æ­¥é•¿
        inverse_r2_dB=None,     # (1/r^2)_dB
        nu_dB=None,             # å›ºå®š Q=-10dB æ—¶ï¼šnu_dB = inverse_r2_dB - 10
        order=1,
        device="cpu",
        x0=None, P0=None
    ):
        N, T, n_obs = Y.shape
        if H is None:
            # è‹¥æ²¡ç»™ Hï¼Œåˆ™å‡è®¾ n_states == n_obsï¼Œç”¨å•ä½é˜µ
            n_states = X_gt.shape[2] if X_gt is not None else n_obs
            H = np.eye(n_obs, n_states, dtype=float)
        else:
            n_states = H.shape[1]

        # ç›´æ¥ç”¨ç±»åå®ä¾‹åŒ–ï¼›ä¸å† import è‡ªå·±
        ekf = EKF_Lorenz(n_states=n_states, n_obs=n_obs,
                         H_mat=H, delta_t=delta_t,
                         inverse_r2_dB=inverse_r2_dB, nu_dB=nu_dB,
                         order=order, device=device)

        X_hat = np.zeros((N, T, n_states), dtype=float)
        P_hat = np.zeros((N, T, n_states, n_states), dtype=float)
        mse_list, nis_list = [], []

        for i in range(N):
            # ---- æ›´ç¨³å¦¥çš„é»˜è®¤åˆå§‹åŒ–ï¼šç”¨é¦–å¸§è§‚æµ‹ä¼°ä¸ª x0ï¼›P0 å–å¤§ä¸€ç‚¹ ----
            if x0 is None:
                y0 = Y[i, 0]
                # H=I æ—¶ x0=y0ï¼›ä¸€èˆ¬æƒ…å½¢ç”¨ä¼ªé€†
                x0_i = y0 if (H.shape[0] == H.shape[1] and np.allclose(H, np.eye(n_obs))) \
                       else np.linalg.pinv(H) @ y0
            else:
                x0_i = x0
            P0_i = (np.eye(n_states) * 5.0) if (P0 is None) else P0

            ekf.initialize(x0=x0_i, P0=P0_i)

            Xhat_i, Phat_i, stats = ekf.filter_sequence(
                Y=torch.from_numpy(Y[i]).float(),
                X_gt=(torch.from_numpy(X_gt[i]).float() if X_gt is not None else None)
            )
            X_hat[i] = Xhat_i.cpu().numpy()
            P_hat[i] = Phat_i.cpu().numpy()
            nis_list.append(stats["NIS_mean"])
            if "mse_lin" in stats: mse_list.append(stats["mse_lin"])
        # æ±‡æ€»è¾“å‡ºï¼Œ if æœ‰ X åˆ™åŠ ä¸Š,  å¦åˆ™åªè¾“å‡º /hat{x}, /hat{P}, NIS
        out = {
            "X_hat": X_hat,
            "P_hat": P_hat,
            "NIS_mean": float(np.mean(nis_list)),
        }
        if mse_list:
            mse_lin = float(np.mean(mse_list))
            out.update({
                "MSE_lin": mse_lin,
                "MSE_dB": 10*np.log10(max(mse_lin, 1e-20))
            })
        return out

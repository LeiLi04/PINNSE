"""
===============================================================================
File: ekf_lorenz.py
Author: You (with ChatGPT assist, 2025)

ğŸ¥ Whatâ€™s new vs EKF_Aliter:
- ä¸å†ä¾èµ– FilterPyï¼›å…¨éƒ¨ç”¨ PyTorch/Numpy è‡ªå·±å®ç°ï¼ˆä¾¿äº CUDAã€å¯æ§æ•°å€¼ç¨³å®šï¼‰ã€‚
- ç»Ÿä¸€æ”¯æŒ dB å‚æ•°ï¼šinverse_r2_dB ä¸ nu_dB -> R, Qã€‚
- é¢„æµ‹æ­¥å¯é€‰ä¸€é˜¶/äºŒé˜¶ç¦»æ•£åŒ–ï¼ˆorder=1/2ï¼‰ï¼›æ›´æ–°æ­¥ç”¨ Joseph å½¢å¼ã€‚
- Cholesky ç™½åŒ–æ±‚è§£ S^{-1}ï¼ˆä¸æ˜¾å¼æ±‚é€†ï¼‰ï¼Œå¯é€‰è¾“å‡ºç™½åŒ–åˆ›æ–°/ NIS ä¾¿äºè¯Šæ–­ã€‚
===============================================================================
"""

import math
import pickle
import numpy as np
import torch
from torch import nn

# ---------- utils ----------
def dB_to_lin(x_dB: float) -> float:
    return 10.0 ** (x_dB / 10.0)

def lin_to_dB(x_lin: float, eps: float = 1e-12) -> float:
    return 10.0 * np.log10(max(x_lin, eps))

def mse_loss(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    return torch.mean((a - b) ** 2)

#   ------- Numerical Jacobian (Central Difference) ----------
def numerical_jacobian_central(func, x: np.ndarray, dt: float = None, eps: float = 1e-5):
    x = np.asarray(x, dtype=float)
    n = x.size
    fx = func(x, dt) if dt is not None else func(x)
    fx = np.asarray(fx, dtype=float)
    m = fx.size
    J = np.zeros((m, n), dtype=float)
    for i in range(n):
        x_f = x.copy(); x_b = x.copy()
        x_f[i] += eps;   x_b[i] -= eps
        f_f = func(x_f, dt) if dt is not None else func(x_f)
        f_b = func(x_b, dt) if dt is not None else func(x_b)
        J[:, i] = (np.asarray(f_f) - np.asarray(f_b)) / (2.0 * eps)
    return J

# ---------- Lorenz-63 dynamics + Jacobian ----------
def lorenz_f(x, sigma=10.0, rho=28.0, beta=8.0/3.0):
    x1, x2, x3 = x
    return np.array([
        sigma * (x2 - x1),
        x1 * (rho - x3) - x2,
        x1 * x2 - beta * x3
    ], dtype=float)

def lorenz_jacobian(x, sigma=10.0, rho=28.0, beta=8.0/3.0):
    x1, x2, x3 = x
    return np.array([
        [-sigma,     sigma,   0.0],
        [ rho - x3, -1.0,   -x1  ],
        [   x2,       x1,   -beta]
    ], dtype=float)

# ---------- EKF class ----------
class EKF_Lorenz(nn.Module):
    """
    Extended Kalman Filter for Lorenz-63.
    - f, h: nonlinear transition/measurement. é»˜è®¤ h(x)=H xã€‚
    - F_jac, H_jac: Jacobians. é»˜è®¤ Lorenz çš„è§£æ Jc ä¸å¸¸æ•° Hã€‚
    - order=1/2: é¢„æµ‹æ­¥çš„ä¸€é˜¶/äºŒé˜¶ç¦»æ•£è¿‘ä¼¼ã€‚
    - Joseph form + Cholesky solve.

    Q/R è®¾å®šï¼š
      è‹¥ä¼ å…¥ inverse_r2_dB ä¸ nu_dBï¼Œåˆ™ï¼š
        r^2 = 10^(-inverse_r2_dB/10),  q^2 = 10^((nu_dB - inverse_r2_dB)/10),
        R = r^2 I, Q = q^2 I
      å¦åˆ™ä½¿ç”¨ Q, R æ˜¾å¼å‚æ•°ã€‚
    """
    def __init__(self, n_states=3, n_obs=3,
                 f=lorenz_f, h=None,
                 F_jacobian=lorenz_jacobian, H_mat=None, H_jacobian=None,
                 Q=None, R=None, delta_t=0.01,
                 inverse_r2_dB=None, nu_dB=None,
                 order=1, device="cpu", jitter=1e-6):
        super().__init__()
        self.nx = n_states
        self.nz = n_obs
        self.f = f
        self.h = h if h is not None else (lambda x: (H_mat @ x))
        self.F_jac = F_jacobian
        self.H_jac = H_jacobian if H_jacobian is not None else (lambda x: H_mat)
        self.dt = float(delta_t)
        self.order = int(order)
        self.device = device
        self.jitter = float(jitter)

        # H é»˜è®¤å•ä½é˜µ
        if H_mat is None:
            H_mat = np.eye(self.nz, self.nx, dtype=float)
        self.H = H_mat.astype(float)

        # ä» dB å»ºç«‹ Q/Rï¼ˆè‹¥æœªæä¾›ï¼‰
        if (inverse_r2_dB is not None) and (nu_dB is not None) and (Q is None) and (R is None):
            r2 = 10.0 ** (-inverse_r2_dB / 10.0)
            q2 = 10.0 ** ((nu_dB - inverse_r2_dB) / 10.0)
            Q = q2 * np.eye(self.nx)
            R = r2 * np.eye(self.nz)

        assert Q is not None and R is not None, "Q/R æœªæä¾›ï¼Œä¹Ÿæœªä» dB å‚æ•°æ¨å¯¼ã€‚"

        self.Q = torch.from_numpy(np.asarray(Q)).float().to(device)
        self.R = torch.from_numpy(np.asarray(R)).float().to(device)
        self.H_torch = torch.from_numpy(self.H).float().to(device)

        # æ»¤æ³¢å†…éƒ¨æ€
        self.x = torch.ones(self.nx, device=device).float()
        self.P = torch.eye(self.nx, device=device).float() * 1e-5

    # ----- åˆå§‹åŒ– -----
    def initialize(self, x0=None, P0=None):
        if x0 is None:
            self.x = torch.ones(self.nx, device=self.device).float()
        else:
            self.x = torch.from_numpy(np.asarray(x0)).float().to(self.device)
        if P0 is None:
            self.P = torch.eye(self.nx, device=self.device).float() * 1e-5
        else:
            self.P = torch.from_numpy(np.asarray(P0)).float().to(self.device)


    # =========================Transition State===================================================
    # ----- é¢„æµ‹ -----
    # ---------- EKF Prediction step ----------
    # æ•°å­¦å…¬å¼è¯´æ˜ï¼š
    #
    # çŠ¶æ€é¢„æµ‹ï¼š
    #   ä¸€é˜¶è¿‘ä¼¼ (order=1):
    #       x_{k+1} â‰ˆ x_k + Î”t f(x_k)
    #   äºŒé˜¶è¿‘ä¼¼ (order=2):
    #       x_{k+1} â‰ˆ x_k + Î”t f(x_k) + 1/2 Î”t^2 J(x_k) f(x_k)
    #
    # çŠ¶æ€è½¬ç§»çŸ©é˜µ Fï¼š
    #   ä¸€é˜¶è¿‘ä¼¼ (order=1):
    #       F_k â‰ˆ I + Î”t J(x_k)
    #   äºŒé˜¶è¿‘ä¼¼ (order=2):
    #       F_k â‰ˆ I + Î”t J(x_k) + 1/2 (Î”t J(x_k))^2
    #
    # åæ–¹å·®ä¼ æ’­ï¼š
    #       P_{k+1} = F_k P_k F_k^T + Q
    #
    # å…¶ä¸­ï¼š
    #   f(x)   : è¿ç»­æ—¶é—´åŠ¨åŠ›å­¦ (Lorenz-63)
    #   J(x)   : f(x) çš„ Jacobian
    #   Î”t     : æ—¶é—´æ­¥é•¿ self.dt
    #   Q      : è¿‡ç¨‹å™ªå£°åæ–¹å·®
    def predict_step(self):
        x_np = self.x.detach().cpu().numpy()
        f = self.f(x_np)                       # R^nx
        J = self.F_jac(x_np)                  # (nx,nx)
        I = torch.eye(self.nx, device=self.device).float()
        J_t = torch.from_numpy(J).float().to(self.device)

        if self.order == 1:
            F = I + self.dt * J_t  
            x_pred = self.x + self.dt * torch.from_numpy(f).float().to(self.device)
        elif self.order == 2:
            Jdt = self.dt * J_t
            F = I + Jdt + 0.5 * (Jdt @ Jdt)
            x_pred = self.x + self.dt * torch.from_numpy(f).float().to(self.device) \
                     + 0.5 * (self.dt**2) * (J_t @ torch.from_numpy(f).float().to(self.device))
        else:
            raise ValueError("order must be 1 or 2")

        P_pred = F @ self.P @ F.T + self.Q
        self.x = x_pred
        self.P = 0.5 * (P_pred + P_pred.T)    # å¯¹ç§°åŒ–ï¼Œé¿å…æ•°å€¼æ¼‚ç§»

        return F  # ä¾¿äºè°ƒè¯•/ä¿å­˜

    # ----- æ›´æ–°ï¼ˆJoseph + Choleskyï¼‰ -----
    # ---------- åæ–¹å·®æ›´æ–° (Joseph form) ----------
    # æ•°å­¦å…¬å¼ï¼š
    #
    #   P_k+ = (I - K_k H_k) P_k- (I - K_k H_k)^T + K_k R_k K_k^T
    #
    # å…¶ä¸­ï¼š
    #   P_k- : å…ˆéªŒåæ–¹å·®
    #   P_k+ : åéªŒåæ–¹å·®
    #   K_k  : å¡å°”æ›¼å¢ç›Š
    #   H_k  : è§‚æµ‹çŸ©é˜µ
    #   R_k  : è§‚æµ‹å™ªå£°åæ–¹å·®

    def update_step(self, z_k: np.ndarray, return_diag=False):
        H = self.H_torch
        z_pred = H @ self.x if self.h is None else torch.from_numpy(self.h(self.x.detach().cpu().numpy())).float().to(self.device)
        innov = torch.from_numpy(np.asarray(z_k)).float().to(self.device) - z_pred

        S = H @ self.P @ H.T + self.R
        S = 0.5 * (S + S.T) + self.jitter * torch.eye(self.nz, device=self.device)
        L = torch.linalg.cholesky(S)
        
        # è®¡ç®— Kalman å¢ç›Š
        # K = P H^T S^{-1} via Cholesky
        HP = H @ self.P                      # (nz,nx)
        U = torch.cholesky_solve(HP, L)      # S^{-1} (H P)
        K = U.T                              # (nx,nz)

        # æ›´æ–°
        self.x = self.x + K @ innov
        I = torch.eye(self.nx, device=self.device)
        P_post = (I - K @ H) @ self.P @ (I - K @ H).T + K @ self.R @ K.T
        self.P = 0.5 * (P_post + P_post.T)

        if return_diag:
            # ç™½åŒ–åˆ›æ–°ä¸ NISï¼ˆä¾¿äºè¯Šæ–­ï¼‰
            nu = torch.cholesky_solve(innov.unsqueeze(1), L).squeeze(1)  # S^{-1} innov
            NIS = (innov * nu).sum()
            return K, L, innov, NIS
        return K

    # ----- è·‘ä¸€æ¡åºåˆ— -----
    @torch.no_grad()
    def filter_sequence(self, Y: torch.Tensor, X_gt: torch.Tensor = None):
        """
        Y: [T, nz];  X_gt: [T, nx] (optional, ä»…ç”¨äºè¯„ä¼°)
        è¿”å›ï¼š
          X_hat: [T, nx]  åéªŒå‡å€¼
          P_hat: [T, nx, nx]  åéªŒåæ–¹å·®
          stats: dict (å¯å« MSE/NIS)
        çº¦å®šï¼šåœ¨ t=0 ä½¿ç”¨åˆå§‹åŒ– (x0,P0)ï¼Œä»ç¬¬ä¸€å¸§è§‚æµ‹ Y[0] å¼€å§‹ predict+updateã€‚
        """
        if Y.dim() == 1:
            Y = Y.unsqueeze(0)
        T = Y.shape[0]

        X_hat = torch.zeros((T, self.nx), device=self.device).float()
        P_hat = torch.zeros((T, self.nx, self.nx), device=self.device).float()
        nis_list = []

        for t in range(T):
            self.predict_step()
            _, L, innov, NIS = self.update_step(Y[t].cpu().numpy(), return_diag=True)
            nis_list.append(NIS.item())
            X_hat[t] = self.x
            P_hat[t] = self.P

        stats = {}
        if X_gt is not None:
            X_gt = X_gt.to(self.device)
            mse_lin = mse_loss(X_gt, X_hat)
            stats["mse_lin"] = mse_lin.item()
            stats["mse_dB"] = lin_to_dB(mse_lin.item())
        stats["NIS_mean"] = float(np.mean(nis_list))
        return X_hat, P_hat, stats


    @staticmethod
    @torch.no_grad()
    def run_ekf_on_dataset(
        Y,                      # np.array [N, T, n_obs]
        X_gt=None,              # np.array [N, T, n_states]ï¼ˆè¯„ä¼°ç”¨ï¼Œå¯ Noneï¼‰
        H=None,                 # np.array [n_obs, n_states]ï¼Œé»˜è®¤ I
        delta_t=0.01,           # æ—¶é—´æ­¥é•¿
        inverse_r2_dB=None,     # (1/r^2)_dB
        nu_dB=None,             # å›ºå®š Q=-10dB æ—¶ï¼šnu_dB = inverse_r2_dB - 10
        order=1,
        device="cpu",
        x0=None, P0=None
    ):
        N, T, n_obs = Y.shape
        if H is None:
            # è‹¥æ²¡ç»™ Hï¼Œåˆ™å‡è®¾ n_states == n_obsï¼Œç”¨å•ä½é˜µ
            n_states = X_gt.shape[2] if X_gt is not None else n_obs
            H = np.eye(n_obs, n_states, dtype=float)
        else:
            n_states = H.shape[1]

        # ç›´æ¥ç”¨ç±»åå®ä¾‹åŒ–ï¼›ä¸å† import è‡ªå·±
        ekf = EKF_Lorenz(n_states=n_states, n_obs=n_obs,
                         H_mat=H, delta_t=delta_t,
                         inverse_r2_dB=inverse_r2_dB, nu_dB=nu_dB,
                         order=order, device=device)

        X_hat = np.zeros((N, T, n_states), dtype=float)
        P_hat = np.zeros((N, T, n_states, n_states), dtype=float)
        mse_list, nis_list = [], []

        for i in range(N):
            # ---- æ›´ç¨³å¦¥çš„é»˜è®¤åˆå§‹åŒ–ï¼šç”¨é¦–å¸§è§‚æµ‹ä¼°ä¸ª x0ï¼›P0 å–å¤§ä¸€ç‚¹ ----
            if x0 is None:
                y0 = Y[i, 0]
                # H=I æ—¶ x0=y0ï¼›ä¸€èˆ¬æƒ…å½¢ç”¨ä¼ªé€†
                x0_i = y0 if (H.shape[0] == H.shape[1] and np.allclose(H, np.eye(n_obs))) \
                       else np.linalg.pinv(H) @ y0
            else:
                x0_i = x0
            P0_i = (np.eye(n_states) * 5.0) if (P0 is None) else P0

            ekf.initialize(x0=x0_i, P0=P0_i)

            Xhat_i, Phat_i, stats = ekf.filter_sequence(
                Y=torch.from_numpy(Y[i]).float(),
                X_gt=(torch.from_numpy(X_gt[i]).float() if X_gt is not None else None)
            )
            X_hat[i] = Xhat_i.cpu().numpy()
            P_hat[i] = Phat_i.cpu().numpy()
            nis_list.append(stats["NIS_mean"])
            if "mse_lin" in stats: mse_list.append(stats["mse_lin"])
        # æ±‡æ€»è¾“å‡ºï¼Œ if æœ‰ X åˆ™åŠ ä¸Š,  å¦åˆ™åªè¾“å‡º /hat{x}, /hat{P}, NIS
        out = {
            "X_hat": X_hat,
            "P_hat": P_hat,
            "NIS_mean": float(np.mean(nis_list)),
        }
        if mse_list:
            mse_lin = float(np.mean(mse_list))
            out.update({
                "MSE_lin": mse_lin,
                "MSE_dB": 10*np.log10(max(mse_lin, 1e-20))
            })
        return out


def _load_lorenz_dataset(dataset_path):
    """Load trajectory pickle generated on another host (handles numpy._core rename)."""
    class _CompatUnpickler(pickle.Unpickler):  # local helper to keep scope tight
        def find_class(self, module, name):
            if module.startswith("numpy._core"):
                module = module.replace("numpy._core", "numpy.core")
            return super().find_class(module, name)

    with open(dataset_path, "rb") as handle:
        return _CompatUnpickler(handle).load()


if __name__ == "__main__":
    import argparse
    from pathlib import Path

    parser = argparse.ArgumentParser(description="EKF_Lorenz quick-look demo on a stored dataset.")
    parser.add_argument(
        "--dataset",
        default="src/data/trajectories/trajectories_m_3_n_3_LorenzSSM_data_T_1000_N_500_r2_20.0dB_nu_-10.0dB.pkl",
        help="Path to the trajectory pickle (default: provided Lorenz dataset).",
    )
    parser.add_argument("--index", type=int, default=0, help="Sample index to visualise (default: 0).")
    parser.add_argument("--order", type=int, default=1, choices=(1, 2), help="Prediction Taylor order.")
    args = parser.parse_args()

    dataset_path = Path(args.dataset)
    if not dataset_path.exists():
        raise FileNotFoundError(f"Dataset not found: {dataset_path}")

    payload = _load_lorenz_dataset(dataset_path)
    num_samples = payload["num_samples"]
    if not (0 <= args.index < num_samples):
        raise IndexError(f"index {args.index} out of range (0 â‰¤ idx < {num_samples}).")

    sample_X, sample_Y = payload["data"][args.index]
    sample_X = np.asarray(sample_X, dtype=float)
    sample_Y = np.asarray(sample_Y, dtype=float)

    ekf = EKF_Lorenz(
        n_states=sample_X.shape[1],
        n_obs=sample_Y.shape[1],
        delta_t=0.02,
        inverse_r2_dB=0.0,
        nu_dB=-10.0,
        order=args.order,
        device="cpu",
    )

    ekf.initialize(x0=sample_Y[0], P0=np.eye(sample_X.shape[1]) * 5.0)

    torch_Y = torch.from_numpy(sample_Y).float()
    torch_X = torch.from_numpy(sample_X[1:]).float()
    X_hat, _, stats = ekf.filter_sequence(torch_Y, torch_X)

    x_hat_np = X_hat.cpu().numpy()
    mse_per_step = np.mean((x_hat_np - sample_X[1:]) ** 2, axis=1)

    time_axis = np.arange(sample_Y.shape[0])
    '''
    #     Project / Algorithm èƒŒæ™¯:
    #     éçº¿æ€§ç³»ç»Ÿ(Lorenz)ä¸‹çš„çŠ¶æ€ä¼°è®¡å¯è§†åŒ–ï¼šå±•ç¤ºçœŸå€¼è½¨è¿¹ã€è§‚æµ‹æ•°æ®ä¸æ»¤æ³¢ä¼°è®¡ï¼Œå¹¶ç»˜åˆ¶é€æ­¥å‡æ–¹è¯¯å·®æ›²çº¿ã€‚

    # Inputs / è¾“å…¥:
    #     sample_X : ndarray, shape [T, 3]
    #         True state trajectory x(t). åˆ—ä¸º (x1, x2, x3)ã€‚
    #     sample_Y : ndarray, shape [T, 3]
    #         Observation sequence y(t)ã€‚ä¸çŠ¶æ€ç»´åº¦ä¸€è‡´çš„è§‚æµ‹å‘é‡ã€‚
    #     x_hat_np : ndarray, shape [T, 3]
    #         Estimated state trajectory x_hat(t)ï¼Œæ¥è‡ª EKF/UKF/Transformer-Filter ç­‰ä¼°è®¡å™¨ã€‚
    #     time_axis : ndarray, shape [T]
    #         ç¦»æ•£æ—¶é—´ç´¢å¼•ï¼Œç”¨äºæ¨ªè½´ç»˜åˆ¶ MSE æ›²çº¿ã€‚
    #     mse_per_step : ndarray, shape [T]
    #         æ¯ä¸ªæ—¶é—´æ­¥çš„çŠ¶æ€ä¼°è®¡å‡æ–¹è¯¯å·®ï¼Œå®šä¹‰å¦‚ mean((x_hat - x_true)^2) over state dimsã€‚
    #     stats : dict
    #         æ±‡æ€»ç»Ÿè®¡é‡ï¼Œä¾‹å¦‚:
    #           - 'NIS_mean' : å¹³å‡ NIS æŒ‡æ ‡ï¼ˆè‹¥ä½¿ç”¨ä¸€è‡´æ€§æ£€éªŒï¼‰
    #           - 'mse_lin'  : çº¿æ€§å°ºåº¦çš„æ•´ä½“ MSE (ä¾‹å¦‚å…¨æ—¶åºå¹³å‡)
    #           - 'mse_dB'   : ä»¥ 10*log10(mse_lin) è¡¨ç¤ºçš„ dB å°ºåº¦è¯¯å·®

    # Outputs / è¾“å‡º:
    #     - "lorenz_ekf_states.png": 3D è½¨è¿¹å›¾ (çœŸå€¼ / è§‚æµ‹ / ä¼°è®¡)ã€‚
    #     - "lorenz_ekf_mse.png"   : é€æ­¥ MSE æ›²çº¿å›¾ã€‚
    #     - æ§åˆ¶å°æ‰“å°ç»Ÿè®¡æ‘˜è¦ (NIS_mean, MSE_lin, MSE_dB)ã€‚

    # Tensor Dimensions / å¼ é‡ç»´åº¦å¯¹ç…§:
    #     T : æ—¶é—´æ­¥é•¿åº¦ï¼Œstate_dim = 3
    #     sample_X[t]     âˆˆ R^3
    #     sample_Y[t]     âˆˆ R^3
    #     x_hat_np[t]     âˆˆ R^3
    #     mse_per_step[t] âˆˆ R
    #     time_axis[t]    âˆˆ R

    # Math Notes / æ•°å­¦è¯´æ˜ (çº¯æ–‡æœ¬å…¬å¼):
    #     per-step MSE:
    #         mse[t] = mean( (x_hat_np[t, :] - sample_X[t, :])^2 )
    #     Summary metrics (ç¤ºä¾‹):
    #         mse_lin = mean_t mse[t]
    #         mse_dB  = 10 * log10( mse_lin )
    #     ä»¥ä¸Šä»…ç¤ºæ„ï¼Œå…·ä½“è®¡ç®—åº”ä¸ä¸Šæ¸¸ç»Ÿè®¡æ¨¡å—ä¿æŒä¸€è‡´ã€‚

    # Target Audience / ç›®æ ‡è¯»è€…:
    #     ç†Ÿæ‚‰æ•°å€¼è®¡ç®—ä¸ä¿¡å·å¤„ç†çš„ç ”ç©¶è€…ä¸å·¥ç¨‹å¸ˆï¼Œå¸Œæœ›å¤ç°å®éªŒå›¾ä¸è¯„ä¼°æŒ‡æ ‡ã€‚
    '''
    
# ==========================================================================
# STEP 01: é…ç½® Matplotlib åç«¯ä¸å…¼å®¹æ€§è¡¥ä¸
# ==========================================================================
# è¯´æ˜:
# - ä½¿ç”¨ "Agg" åç«¯ä»¥ä¾¿åœ¨æ— æ˜¾ç¤ºç¯å¢ƒ(å¦‚æœåŠ¡å™¨/CI/Colab åå°)ç”Ÿæˆå¹¶ä¿å­˜é™æ€å›¾åƒã€‚
# - ä¸€äº›è€ç‰ˆæœ¬/ç‰¹å®šå‘è¡Œç‰ˆçš„ matplotlib.cbook ä¸­ç¼ºå¤±å†…éƒ¨ç¬¦å·ã€‚
#   é€šè¿‡ hasattr æ£€æŸ¥å¹¶æ³¨å…¥æœ€å°æ›¿ä»£å®ç°ï¼Œç¡®ä¿åç»­ä»£ç åœ¨å¤šç¯å¢ƒä¸‹å¯è¿è¡Œã€‚
# æ³¨æ„:
# - è¿™äº›è¡¥ä¸æ˜¯å·¥ç¨‹æ€§å…¼å®¹æªæ–½ï¼Œä¸æ”¹å˜ç»˜å›¾è¯­ä¹‰ï¼Œä»…é¿å… AttributeErrorã€‚
import matplotlib
matplotlib.use("Agg")

import matplotlib.cbook as _mpl_cbook

if not hasattr(_mpl_cbook, "_is_pandas_dataframe"):
    # ----------------------------------------------------------------------
    # è¡¥ä¸ 1: æ ‡è®°å¯¹è±¡æ˜¯å¦ä¸º pandas.DataFrame
    # åœ¨æŸäº›ç‰ˆæœ¬ä¸­è¯¥å†…éƒ¨å·¥å…·å‡½æ•°ä¸å­˜åœ¨ï¼Œå®šä¹‰ä¸€ä¸ªä¿å®ˆè¿”å› False çš„æ›¿ä»£ã€‚
    def _is_pandas_dataframe(_obj):
        return False

    _mpl_cbook._is_pandas_dataframe = _is_pandas_dataframe

if not hasattr(_mpl_cbook, "_Stack") and hasattr(_mpl_cbook, "Stack"):
    # ----------------------------------------------------------------------
    # è¡¥ä¸ 2: å°†å…¬å¼€çš„ Stack æ˜ å°„ä¸ºå†…éƒ¨å _Stackï¼Œæ»¡è¶³æ—§ä»£ç å¼•ç”¨ã€‚
    _mpl_cbook._Stack = _mpl_cbook.Stack

if not hasattr(_mpl_cbook, "_ExceptionInfo"):
    # ----------------------------------------------------------------------
    # è¡¥ä¸ 3: å®šä¹‰ _ExceptionInfo å‘½åå…ƒç»„ï¼Œä¾¿äºç»Ÿä¸€å¼‚å¸¸ä¿¡æ¯å°è£…ã€‚
    from collections import namedtuple

    _mpl_cbook._ExceptionInfo = namedtuple("_ExceptionInfo", "exc_class exc traceback")

if not hasattr(_mpl_cbook, "_broadcast_with_masks"):
    # ----------------------------------------------------------------------
    # è¡¥ä¸ 4: å®šä¹‰ _broadcast_with_masksï¼Œæœ€å°å®ç°ä»…è¿›è¡Œ numpy å¹¿æ’­ã€‚
    # ç”¨é€”: å¯¹è¾“å…¥æ•°ç»„è¿›è¡Œå½¢çŠ¶å¹¿æ’­ï¼Œè¿”å›å¹¿æ’­åçš„å…ƒç»„ã€‚
    import numpy as np
    def _broadcast_with_masks(*arrays):
        broadcasted = np.broadcast_arrays(*[np.asarray(arr) for arr in arrays])
        return tuple(broadcasted)

    _mpl_cbook._broadcast_with_masks = _broadcast_with_masks

# ==========================================================================
# STEP 02: å¯¼å…¥ç»˜å›¾ API ä¸ 3D å·¥å…·
# ==========================================================================
# - pyplot: é¢å‘çŠ¶æ€æœºçš„ç»˜å›¾æ¥å£ï¼Œä¾¿æ·ç”Ÿæˆ Figure/Axesã€‚
# - mpl_toolkits.mplot3d: ä»¥ 3D æŠ•å½±ç»˜åˆ¶ Lorenz è½¨è¿¹ï¼ˆx1, x2, x3ï¼‰ã€‚
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 (projection side-effects)

# ==========================================================================
# STEP 03: åˆ›å»ºç”»å¸ƒä¸ä¸‰ä¸ª 3D å­å›¾ (çœŸå€¼ / è§‚æµ‹ / ä¼°è®¡)
# ==========================================================================
# å¸ƒå±€:
#   [1, 3] çš„å­å›¾æ …æ ¼:
#     - ax_true:  çœŸå€¼ x(t)
#     - ax_obs:   è§‚æµ‹ y(t)
#     - ax_est:   ä¼°è®¡ x_hat(t)
# è¯´æ˜:
#   - figsize é‡‡ç”¨æ¨ªå‘å®½å±ï¼Œä¾¿äºå¹¶æ’æ¯”è¾ƒä¸‰æ¡è½¨è¿¹çš„å‡ ä½•å·®å¼‚ã€‚
fig = plt.figure(figsize=(18, 5))
ax_true = fig.add_subplot(1, 3, 1, projection="3d")
ax_obs = fig.add_subplot(1, 3, 2, projection="3d")
ax_est = fig.add_subplot(1, 3, 3, projection="3d")

# ==========================================================================
# STEP 04: ç»˜åˆ¶çœŸå€¼è½¨è¿¹ x(t)
# ==========================================================================
# æ•°æ®çº¦å®š:
#   sample_X[1:, i] å¯¹åº”ç¬¬ i ä¸ªçŠ¶æ€åˆ†é‡çš„æ—¶é—´åºåˆ—ï¼Œi âˆˆ {0,1,2}
# ç»†èŠ‚:
#   - ä» 1 å¼€å§‹ç´¢å¼•ï¼Œé¿å¼€å¯èƒ½çš„åˆå§‹è¿‡æ¸¡ç‚¹ (è‹¥é¦–ç‚¹ä¸å¯è§†åŒ–)ã€‚
#   - label ç”¨äºå›¾ä¾‹åŒºåˆ†ã€‚
ax_true.plot(sample_X[1:, 0], sample_X[1:, 1], sample_X[1:, 2], label="x (true)", color="tab:blue", linewidth=1.5)
ax_true.set_title("True state x")
ax_true.set_xlabel("x1")
ax_true.set_ylabel("x2")
ax_true.set_zlabel("x3")
ax_true.legend(fontsize="small")

# ==========================================================================
# STEP 05: ç»˜åˆ¶è§‚æµ‹è½¨è¿¹ y(t)
# ==========================================================================
# è¯´æ˜:
#   - è§‚æµ‹ y å¯èƒ½å—æµ‹é‡å™ªå£°/è§‚æµ‹æ¨¡å‹å½±å“ï¼Œä¸çœŸå€¼å­˜åœ¨åå·®ã€‚
#   - é€šè¿‡ 3D è½¨è¿¹å½¢æ€å¯¹æ¯”å¯ç›´è§‚çœ‹åˆ°å™ªå£°ä¸è§‚æµ‹å‡½æ•°çš„å½±å“ã€‚
ax_obs.plot(sample_Y[:, 0], sample_Y[:, 1], sample_Y[:, 2], label="y (obs)", color="tab:green", linewidth=1.2)
ax_obs.set_title("Observation y")
ax_obs.set_xlabel("y1")
ax_obs.set_ylabel("y2")
ax_obs.set_zlabel("y3")
ax_obs.legend(fontsize="small")

# ==========================================================================
# STEP 06: ç»˜åˆ¶ä¼°è®¡è½¨è¿¹ x_hat(t)
# ==========================================================================
# è¯´æ˜:
#   - ä¼°è®¡è½¨è¿¹åº”å°½å¯èƒ½è´´è¿‘çœŸå€¼è½¨è¿¹ï¼›åå·®ä¸å»¶è¿Ÿåæ˜ æ»¤æ³¢å™¨æ€§èƒ½ã€‚
#   - è‹¥é‡‡ç”¨ EKF/UKF/Particle/NN-Filterï¼Œå·®å¼‚æ¥æºåŒ…æ‹¬çº¿æ€§åŒ–è¯¯å·®ã€å™ªå£°å»ºæ¨¡ã€ç½‘ç»œè¡¨è¾¾ç­‰ã€‚
ax_est.plot(x_hat_np[:, 0], x_hat_np[:, 1], x_hat_np[:, 2], label="$\\hat{x}$ (estimate)", color="tab:orange", linewidth=1.5)
ax_est.set_title("Estimate $\\hat{x}$")
ax_est.set_xlabel("x1")
ax_est.set_ylabel("x2")
ax_est.set_zlabel("x3")
ax_est.legend(fontsize="small")

# ä¼˜åŒ–æ•´ä½“å¸ƒå±€ï¼Œé¿å…å­å›¾å…ƒç´ é‡å ï¼›ä¿å­˜çŠ¶æ€è½¨è¿¹å›¾ã€‚
fig.tight_layout()
fig.savefig("lorenz_ekf_states.png", dpi=300)

# ==========================================================================
# STEP 07: ç»˜åˆ¶é€æ­¥ MSE æ›²çº¿
# ==========================================================================
# å«ä¹‰:
#   - mse_per_step[t] åº¦é‡ t æ—¶åˆ»çš„ä¼°è®¡è¯¯å·®å¼ºåº¦ã€‚æ›²çº¿è¶Šä½è¶Šå¥½ã€‚
#   - ä»¥ time_axis ä¸ºæ¨ªè½´ï¼Œæœ‰åŠ©äºå®šä½è¯¯å·®å³°å€¼ä¸ç¨³æ€æ”¶æ•›æ®µã€‚
# è®¾è®¡:
#   - ä½¿ç”¨ç»†è™šçº¿ baseline=0 ä¾¿äºå‚ç…§ã€‚
fig_mse = plt.figure(figsize=(10, 3))
plt.plot(time_axis, mse_per_step, label="per-step MSE", color="tab:red")
plt.axhline(0.0, color="black", linewidth=0.8, linestyle=":", label="zero baseline")
plt.ylabel("MSE")
plt.xlabel("Time step")
plt.title("Per-step state estimation MSE")
plt.legend()
plt.grid(True, linewidth=0.3, alpha=0.7)
fig_mse.tight_layout()
fig_mse.savefig("lorenz_ekf_mse.png", dpi=600)

# ==========================================================================
# STEP 08: æ‰“å°æ±‡æ€»ç»Ÿè®¡
# ==========================================================================
# è¯´æ˜:
#   - NIS_mean: è‹¥å®ç°äº† NIS ä¸€è‡´æ€§æ£€éªŒï¼Œå…¶å‡å€¼åº”æ¥è¿‘çŠ¶æ€ç»´åº¦ (åœ¨ç†æƒ³å‡è®¾ä¸‹)ã€‚
#   - mse_lin / mse_dB: ä¾¿äºåœ¨æ—¥å¿—ä¸­å¿«é€Ÿæ¯”è¾ƒä¸åŒæ¨¡å‹æˆ–è¶…å‚é…ç½®ã€‚
print(
    f"Summary stats -> NIS_mean: {stats['NIS_mean']:.3f}, "
    f"MSE_lin: {stats['mse_lin']:.3f}, MSE_dB: {stats['mse_dB']:.3f}"
)

# ==========================================================================
# STEP 09: åˆ·æ–°ç»˜å›¾
# ==========================================================================
# è¯´æ˜:
#   - å³ä¾¿ä½¿ç”¨ "Agg" åç«¯ï¼Œè°ƒç”¨ plt.show() åœ¨äº¤äº’ç¯å¢ƒä¸‹ä¹Ÿå¯è§¦å‘æ¸²æŸ“ï¼Œ
#     åœ¨æ— æ˜¾ç¤ºç¯å¢ƒä¸­ä¸ä¼šå¼¹çª—ï¼Œä½†ä¸å½±å“å›¾åƒä¿å­˜è‡³æ–‡ä»¶ã€‚
plt.show()






